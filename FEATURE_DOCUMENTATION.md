# Early Decline Signal Detection Engine - Feature Documentation

## üéØ Overview

This feature detects early signs of trend decline using 4 complementary signals:
1. **Engagement Drop** - Sudden loss of user engagement volume
2. **Velocity Decline** - Slowing growth rate or entering decline phase
3. **Creator Decline** - Influencers/creators abandoning the trend
4. **Quality Decline** - Content becoming low-effort/spammy

Each signal is scored 0-100, weighted, and aggregated into a **Risk Score (0-100)** ‚Üí **Alert Level (GREEN/YELLOW/ORANGE/RED)**.

---

## ‚úÖ What's Been Validated

### Logic & Mathematics
- ‚úÖ Signal calculation formulas (20+ unit tests passing)
- ‚úÖ Weighted aggregation (verified with manual math)
- ‚úÖ Growth rate calculation (positive/negative acceleration)
- ‚úÖ Velocity decline detection for both growth slowdown AND collapse
- ‚úÖ Creator exodus detection (% drop + follower quality)
- ‚úÖ Quality degradation (engagement per post + engagement ratio)
- ‚úÖ API response format and validation
- ‚úÖ Fallback mechanism when lifecycle data unavailable

### Bug Fixes Applied
- ‚úÖ **velocity_decline.py**: Fixed max_score capping at 50 (now scales to 100)
- ‚úÖ **velocity_decline.py**: Fixed decline phase detection (now measures decline magnitude, not just acceleration direction)

### Test Coverage
- ‚úÖ 5 real-world scenarios (healthy, collapse, exodus, quality, catastrophic)
- ‚úÖ All assertions passing with EXIT CODE 0
- ‚úÖ Edge cases: insufficient data, zero growth, stable metrics

---

## ‚ö†Ô∏è What's Assumption-Based (Needs Validation)

### Signal Thresholds

| Signal | Sensitivity | Threshold | Source | Confidence | Notes |
|--------|-------------|-----------|--------|------------|-------|
| **Engagement Drop** | VIRAL | 10% | Assumption | 30% | Needs real data validation |
| | PLATEAU | 15% | Assumption | 30% | |
| | DECLINE | 25% | Assumption | 30% | |
| **Velocity Decline** | VIRAL | -0.05 accel | Assumption | 30% | Daily growth rate deceleration |
| | PLATEAU | -0.08 accel | Assumption | 30% | |
| | DECLINE | -0.20 accel | Assumption | 30% | |
| **Creator Decline** | VIRAL | 5% drop | Assumption | 30% | Percentage of creators leaving |
| | PLATEAU | 10% drop | Assumption | 30% | |
| **Quality Decline** | VIRAL | 8% EPP drop | Assumption | 30% | Engagement per post % change |
| | PLATEAU | 12% EPP drop | Assumption | 30% | |

### Alert Level Thresholds

```python
ALERT_LEVELS = {
    (0, 30): "green",      # Safe - all signals low
    (30, 57): "yellow",    # Watch - early warning signs
    (57, 80): "orange",    # Alert - significant decline detected
    (80, 101): "red",      # Critical - multiple system failures
}
```

**Current basis:** Empirical tuning to match 5 test scenarios, NOT validated against production data.

### Signal Weights

```python
SIGNAL_WEIGHTS = {
    "engagement_drop": 0.27,    # User loss (30% of alert)
    "velocity_decline": 0.28,   # Growth slowdown (28% of alert)
    "creator_decline": 0.25,    # Influencer exodus (25% of alert)
    "quality_decline": 0.20,    # Content degradation (20% of alert)
}
```

**Rationale:** Quality elevated from 10% to 20% to ensure spam takeover is detected. Engagement/velocity nearly equal weight as co-indicators of decline.

**Confidence:** 40% - Reasonable starting point, but needs real-world validation.

---

## üìä Lifecycle-Aware Sensitivity

Different trend lifecycle stages have different expectations:

| Lifecycle Stage | Sensitivity | Engagement Threshold | Velocity Threshold | Creator Threshold | Use Case |
|---|---|---|---|---|---|
| Emergence (1) | very_low | 40% drop | -0.15 accel | 30% drop | New trends - lenient |
| Viral (2) | very_high | 10% drop | -0.05 accel | 5% drop | Peak phase - strict |
| Plateau (3) | medium | 15% drop | -0.08 accel | 10% drop | Stabilizing - balanced |
| Decline (4) | low | 25% drop | -0.20 accel | 20% drop | Already declining - forgiving |
| Death (5) | minimal | 50% drop | -0.50 accel | 50% drop | Dead trends - ignore noise |

**Current Implementation:** Hardcoded mappings in `config.py` ‚Üí `STAGE_SENSITIVITY`

**Dependency:** Lifecycle stage comes from Feature #1 (your friend's work). Fallback: assumes PLATEAU stage when unavailable.

---

## üîå Integration Points

### 1. Feature #1: Lifecycle Detection (External Dependency)

**What we expect:**
```python
lifecycle_info = {
    "trend_id": "viral_dance",
    "lifecycle_stage": 2,           # 1-5
    "stage_name": "Viral Explosion",
    "days_in_stage": 5,
    "confidence": 0.95
}
```

**Current handling in `lifecycle_handler.py`:**
```python
def resolve_lifecycle_stage(lifecycle_info):
    if lifecycle_info is None:
        # Fallback: assume PLATEAU (medium sensitivity)
        return 3, "Plateau", "degraded"
    # Parse and validate the incoming lifecycle_info
```

‚úÖ **Works without dependency** - Feature runs independently when Feature #1 is unavailable
‚ö†Ô∏è **Confidence reduced** - Fallback reports `data_quality: "degraded"`

### 2. Database (database.py)

**Current Purpose:**
- Store signal history (last 30 days per trend)
- Power `/history` endpoint
- Persist alerts for analytics

**Status:** ‚ö†Ô∏è **Optional** - Not required for core detection logic
- Core algorithm runs in-memory
- Database adds persistence/audit trail
- Can be stubbed, mocked, or removed if not needed

**Question:** Do you need historical data storage? Or just real-time alerts?

### 3. API Endpoints (main.py, test_api_endpoints.py)

**Implemented:**
```
POST /api/v1/decline-signals
POST /api/v1/decline-signals/latest
GET /api/v1/decline-signals/history/{trend_id}
GET /api/v1/health
```

**Response Format:**
```json
{
  "trend_id": "viral_dance",
  "risk_score": 8.1,
  "alert_level": "green",
  "confidence": "high",
  "data_quality": "complete",
  "signals": {
    "engagement_drop": {
      "score": 0.0,
      "explanation": "Drop: -46.7% (threshold: 10%)"
    },
    "velocity_decline": {
      "score": 16.1,
      "explanation": "Acceleration: -0.0201 (threshold: -0.05)"
    },
    "creator_decline": {
      "score": 0.0,
      "explanation": "Creators: -32.7%, Followers: 0.0%"
    },
    "quality_decline": {
      "score": 24.9,
      "explanation": "EPP: 0.0%, Ratio: 0.0%"
    }
  },
  "timestamp": "2026-02-07T15:30:45Z"
}
```

---

## üß™ Testing & Validation

### Current Test Suite

**test.py** - 5 Real-World Scenarios
- ‚úÖ Scenario 1: Healthy Viral Growth ‚Üí GREEN
- ‚úÖ Scenario 2: Sharp Engagement Collapse ‚Üí RED
- ‚úÖ Scenario 3: Sharp Creator Exodus ‚Üí ORANGE
- ‚úÖ Scenario 4: Sharp Quality Collapse ‚Üí ORANGE
- ‚úÖ Scenario 5: Catastrophic Collapse ‚Üí RED

Run: `python test.py`

### Production Validation Checklist

Before deploying to production, you MUST validate:

- [ ] **False Positive Rate** - Run against 50+ healthy trends
  - Target: < 5% false ORANGE/RED alerts
  - Metric: How many stable trends trigger alert?

- [ ] **False Negative Rate** - Run against 20+ actual collapsed trends
  - Target: > 90% detection rate
  - Metric: How many actual collapses did we miss?

- [ ] **Lead Time** - Measure days-to-alert before actual collapse
  - Target: 3-5 days early warning
  - Metric: When does ORANGE alert fire vs when does trend actually collapse?

- [ ] **Real Metric Ranges** - Verify assumptions match actual data
  - Do healthy trends have 5% creator variance? (we assume <5% is normal)
  - Do viral trends really sustain 20%+ daily growth? (we assume threshold)
  - What's the actual distribution of engagement drop percentages?

- [ ] **Calendar Effects** - Validate against weekday/weekend patterns
  - Do weekends cause false positives? (natural engagement drops)
  - Do holidays trigger unnecessary alerts?

- [ ] **Threshold Sensitivity** - Run A/B test different thresholds
  - Current: GREEN 0-30, YELLOW 30-57, ORANGE 57-80, RED 80+
  - Test: GREEN 0-25, YELLOW 25-55, ORANGE 55-75, RED 75+
  - Measure impact on false positives/negatives

---

## üîß How to Tune Thresholds

### If False Positives Are High (too many orange alerts for healthy trends)

1. **Increase from 10% to 12%:**
   ```python
   ENGAGEMENT_DROP_THRESHOLDS = {
       "very_high": {"drop_percent": 12, ...},  # Was 10
   }
   ```

2. **OR increase ORANGE threshold from 57 to 60:**
   ```python
   ALERT_LEVELS = {
       (0, 30): "green",
       (30, 57): "yellow",
       (57, 80): "orange",  # Requires fewer high signals
   }
   ```

3. **OR reduce quality weight (was too sensitive):**
   ```python
   SIGNAL_WEIGHTS = {
       "quality_decline": 0.15,  # Was 0.20
   }
   ```

### If False Negatives Are High (missing actual collapses)

1. **Decrease from 10% to 8%:**
   ```python
   ENGAGEMENT_DROP_THRESHOLDS = {
       "very_high": {"drop_percent": 8, ...},  # Was 10
   }
   ```

2. **OR decrease ORANGE threshold from 57 to 50:**
   ```python
   ALERT_LEVELS = {
       (0, 30): "green",
       (30, 50): "yellow",     # Lower bar for orange
       (50, 80): "orange",      # Catches more early declines
   }
   ```

3. **OR increase creator weight (they leave first):**
   ```python
   SIGNAL_WEIGHTS = {
       "creator_decline": 0.30,  # Was 0.25
   }
   ```

---

## üìà Real-World Data Requirements

To properly validate this feature, you need:

### Dataset Structure
```python
{
    "trend_id": "viral_dance",
    "daily_metrics": [
        {
            "date": "2026-01-01",
            "total_engagement": 5000000,
            "views": 50000000,
            "posts_count": 200,
            "creators_count": 150,
            "avg_creator_followers": 280000,
            "avg_comments_per_post": 42,
            "avg_engagement_per_post": 46
        },
        # ... 6 more days
    ],
    "lifecycle_info": {
        "lifecycle_stage": 2,
        "confidence": 0.95
    },
    "ground_truth": {
        "collapsed": true,  # Did this trend actually collapse?
        "collapse_date": "2026-01-05",  # When?
        "reason": "negative_sentiment"  # Why?
    }
}
```

### Recommended Sample Size
- **Minimum:** 50 trends (30 healthy, 20 collapsing)
- **Better:** 200 trends (150 healthy, 50 collapsing)
- **Production-ready:** 500+ trends across different categories

---

## üö® Known Limitations

1. **No context awareness** - Algorithm treats all trends identically
   - A 10% engagement drop for a 1M-engagement trend is different from 100M-engagement trend
   - Potential fix: Use relative changes, not absolute percentages

2. **No external signal integration** - Only uses engagement metrics
   - Sentiment analysis? (negative comments can precede collapse)
   - Hashtag velocity? (declining hashtag use = early warning)
   - Media coverage? (mentions dropping = declining interest)

3. **Short data window** - Currently uses 7 days of history
   - Seasonal trends might look declining on weekends (false positive)
   - Very new trends (1-2 days) have insufficient acceleration data
   - Potential fix: Longer rolling windows (14-30 days) or day-of-week normalized

4. **No trend category tuning** - Same thresholds for all trend types
   - Music trends vs political trends have different patterns
   - Potential fix: Category-specific threshold matrices

5. **Lifecycle dependency** - No lifecycle = reduced confidence
   - Currently falls back to PLATEAU assumption
   - Some trends skip VIRAL phase entirely (slow burn)
   - Potential fix: Feature #1 integration mandatory, or improve fallback logic

---

## üìù Configuration File Reference

All thresholds live in `decline_signals/config.py`:

```python
# Lifecycle stages
class LifecycleStage(Enum):
    EMERGENCE = 1
    VIRAL = 2
    PLATEAU = 3
    DECLINE = 4
    DEATH = 5

# Stage sensitivity mapping
STAGE_SENSITIVITY = {
    LifecycleStage.EMERGENCE: "very_low",
    LifecycleStage.VIRAL: "very_high",
    LifecycleStage.PLATEAU: "medium",
    LifecycleStage.DECLINE: "low",
    LifecycleStage.DEATH: "minimal",
}

# Signal-specific thresholds (modify here)
ENGAGEMENT_DROP_THRESHOLDS = {...}
VELOCITY_DECLINE_THRESHOLDS = {...}
CREATOR_DECLINE_THRESHOLDS = {...}
QUALITY_DECLINE_THRESHOLDS = {...}

# Overall weights (modify here)
SIGNAL_WEIGHTS = {...}

# Alert boundaries (modify here)
ALERT_LEVELS = {...}
```

---

## üîÑ Development Workflow

### Adding a New Signal

1. Create `decline_signals/signals/new_signal.py`
2. Implement `calculate_new_signal(daily_metrics, sensitivity, thresholds) ‚Üí (score, explanation)`
3. Add thresholds to `config.py`: `NEW_SIGNAL_THRESHOLDS = {...}`
4. Add weight to `SIGNAL_WEIGHTS`: `"new_signal": 0.XX`
5. Update aggregator: `signal_scores["new_signal"] = calculate_new_signal(...)`
6. Test coverage: Add to `test_decline_signals.py`

### Modifying Thresholds

1. Edit `config.py` thresholds
2. Run: `python test.py` (verify tests still pass)
3. Run against validation dataset (measure false positives/negatives)
4. Commit with reasoning in PR

---

## üéì Signal Interpretation Guide

### What Each Signal Means

**Engagement Drop (30% weight)**
- Indicates: Raw user loss
- Triggers on: 10-50% volume decrease vs baseline
- Why it matters: Direct measurement of audience size leaving
- False positive risk: High on weekends/holidays

**Velocity Decline (28% weight)**
- Indicates: Growth rate slowing or entering decline
- Triggers on: Negative acceleration (-0.05 to -0.50 depending on stage)
- Why it matters: Earliest warning before visible collapse
- False positive risk: Medium - natural trends plateau

**Creator Decline (25% weight)**
- Indicates: Influencers abandoning trend
- Triggers on: 5-50% creator exodus vs baseline
- Why it matters: Leads market - creators leave before audiences
- False positive risk: Low - creators usually stay in healthy trends

**Quality Decline (20% weight)**
- Indicates: Content becoming low-effort/spammy
- Triggers on: 8-40% engagement-per-post decrease
- Why it matters: Spammy trends collapse: faster than quality ones
- False positive risk: Medium - some natural variance

### When to Act on Each Alert Level

| Level | Score | Action | Timeline |
|-------|-------|--------|----------|
| GREEN | 0-30 | Monitor normally | Daily checks |
| YELLOW | 30-57 | Increase vigilance | 2x daily checks |
| ORANGE | 57-80 | Escalate to content team | Investigate within 24h |
| RED | 80-100 | Immediate intervention | Immediate (within 1h) |

---

## üìû Support & Questions

**Architecture Questions:**
- See `ARCHITECTURE.md` (if created)

**Threshold Tuning:**
- Run: `python test.py` with modified config.py
- Validate against your validation dataset

**Integration with Feature #1:**
- Coordinate with your friend on `lifecycle_info` schema
- See `lifecycle_handler.py` for expected format

**Real-world Performance:**
- Document findings in a `VALIDATION_REPORT.md`
- Share false positive/negative rates with team

---

## ‚ú® Summary

**What's Production-Ready:**
- ‚úÖ Core detection logic
- ‚úÖ Signal calculation formulas
- ‚úÖ API endpoints
- ‚úÖ Fallback mechanisms

**What Needs Pre-Production Validation:**
- ‚ö†Ô∏è Threshold accuracy
- ‚ö†Ô∏è Alert level boundaries
- ‚ö†Ô∏è Signal weights
- ‚ö†Ô∏è False positive/negative rates

**Next Steps:**
1. Run against 50+ real trends
2. Measure accuracy metrics
3. Tune thresholds iteratively
4. Document findings
5. Deploy with confidence

Good luck! üöÄ
